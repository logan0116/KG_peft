# GlaM

## 研究问题

* 当前大型语言模型（LLMs）在对话和文本生成方面表现出色，但它们在通过领域特定的知识图谱进行多步推理时的能力有限。尤其是，在私有数据库中基于实体间的关系和属性来识别特定目标的最优联系点的能力尚不足够。

## 研究Gap

* 现有方法通常将知识库视为外部可检索的存储，而不是将其整合到模型参数中。虽然有尝试使用LLM作为编码器来转换图中节点和边的文本标签，并与图神经网络（GNN）派生的表示进行融合的研究，但**直接通过微调将领域特定知识图谱整合进LLM表示中的研究仍然较少**。

## 研究内容

* 介绍了一种微调框架，用于开发Graph-aligned Language Models（GLaM），该框架**将知识图谱转换为带标签的问题-答案对**的替代文本表示形式。这种方法旨在扩展模型的结构化推理能力，并提出了一种高效的替代方法来增强检索增强生成风格的方法。

## 创新点

* 提出了一种新颖的微调框架，使得LLMs能够直接利用领域特定知识图谱，以提高开放式问答任务的准确性。这种方法通过编码图谱的架构和实体，增强了模型在真实世界约束下的多跳推理能力。
* 通过生成数据集的大语言模型的生成能力，提出了一种高效的方法，旨在替代传统的检索增强生成方法。

## 贡献

* 第一次研究了**通过微调将领域特定知识图谱直接整合进LLM表示中的方法**，目标是提高开放式问答(QA)的准确性，这是一项比先前工作中探索的多选题设置更复杂的任务。
* 解决了自由形式推理中的事实性幻觉问题，同时保持了文本处理的多功能性。
* 展示了在结构化推理方面，通过特定图谱知识基础的LLMs能够实现的性能提升，特别是在节点分类和链接预测等任务上的神经网络性能改进。


## 方法综述

1. 委托给图神经网络（GNN）
   * 方法概述：这种常见方法使用GNN作为编码器，从自然语言查询中提取实体和关系，并整合GNN与LLM的表示。 
   * 实现方式：整合可以通过学习将LLM和GNN表示耦合的联合模型，或使用软提示方法，将GNN派生的向量嵌入到LLM提示中。
2. 检索增强生成（RAG） 
   * 方法概述：RAG方法与委托给GNN类似，但是它查询的是包含节点和/或关系嵌入的外部图数据库或向量数据库，而不是使用GNN。 
   * 实现方式：LLM用作向本地图数据库或机器学习模型的路由接口，相应图形组件的答案被反馈给用户，LLM作为生成层产生最终答案。
3. 少样本提示
   * 方法概述：这种方法提取与查询Q相关的子图，并将它们连同示例一起插入到提示中。 
   * 潜在缺陷：可能需要在LLM提示中编码完整图或为每个问题执行多跳子图检索，这是一个挑战。
4. 微调的动机
    * 微调与其他方法的不同：与上述方法相比，微调直接将领域知识嵌入到模型参数和表示中，而不是将图视为外部附加组件。通过直接编码约束和依赖关系到知识基底，微调允许在模型认知的每一步中受到上下文图的影响。 
    * 优点：微调允许更紧密、更细粒度地形成复杂推理，图形不仅仅是一个静态查找，而是成为一个整体推理组成部分。


## 本文方法优势

1. 邻域分割和编码方案
   * 改进点：与简单的图数据编码或直接检索不同，作者提出的邻域分割和编码方案能够适应真实世界图的属性，如偏斜的大小分布和稀疏性。这种方法为如何根据成本-准确度权衡设置上下文大小限制，为LLMs调整编码开辟了新的实验可能性。 
   * 优势：能够更灵活地处理不同类型和规模的图数据，提高模型处理复杂图结构的能力，同时允许更精细地调整模型性能和资源消耗之间的平衡。
2. 多种编码方法的探索和评估
   * 改进点：作者不仅提出了一种编码策略，而是评估了**五种利用LLM固有的概括和文本生成强项的编码方法**。例如，通过评估LLM生成的邻域摘要，作者的方法与同时期工作（如Fatemi, Halcrow, 和 Perozzi 2023）相呼应，证实了这一方向的潜力。 
   * 优势：提供了一种更加全面和系统的方式来探索和利用LLMs在处理和概括复杂知识图谱数据方面的能力，增强了模型在不同情境下的适应性和效果。
3. 开发新的领域问答数据集
   * 改进点：基于两个图（UMLS和DBLP）开发的领域问答数据集，涵盖了从**链接预测到多跳推理查询**的一系列评估任务，提供了对该方法效果的全面评估。 
   * 优势：通过这些数据集，研究社区不仅能够评估提出方法的有效性，还能探索和开发新的图数据处理和推理技术。数据集和代码的开源计划进一步促进了这一领域的研究和创新。


## 模型

### 1. 算法概述
* 输入：知识图谱G（包含节点集合V和边集合E），以及上下文子图节点限制N_max。
* 输出：微调数据集D。

### 2. 数据集生成步骤
* 提取k跳邻域子图：对于图G中的每个节点 v，使用查询函数f_aggr(G,v,k)检索v的k跳邻域子图G_context(v,k)。
* 邻域子图的分区：根据节点计数限制N_max，将G_context(v,k)分区，以保证分区后的文本序列长度不会超过LLMs的最大令牌限制T_max。
* 邻域编码：将G_context(v,k)或其分区后的子图编码成文本，通过函数f_enc(gsub)实现。
* 生成QA对：使用G_context(v,k)生成需要推理的问题-答案对，通过函数f_qa(gsub)实现。
* 数据集合成：将f_enc(G,v)和f_qa(G,v)的输出合并为一个文本序列，然后加入到微调数据集D中。

### 3. 邻域编码函数
* 目的：将围绕节点v的k跳邻域子图G_context(v,k)翻译成文本表示形式，以便LLMs能够有效处理并进行高阶推理。
* 因素：需要有效传达图结构和高阶逻辑依赖，同时与LLMs的内部知识表示语义对齐。

### 4. 编码方法
* 通过三元组编码：将边数据翻译成（源，关系，目标）三元组，为LLMs提供节点间关系的基本信息。
* 通过邻接列表/关系组编码：更新邻域编码以包括关于子图中多个节点的信息，例如包含中心节点的整个邻接列表，或根据它们的关系类型将邻居分区成子集。
* 通过概括编码：使用提示方法将上述方法编码的信息重写为更连贯的表示形式，以实现语义对齐。
* 通过节点描述符编码：利用LLMs的零样本能力，将k跳邻域子图转换为基于文本的节点描述符集。

### 5. 生成问题-答案对
给定从子图G_context(v,k)生成的文本上下文，通过提示不同任务（事实回顾、逆事实回顾、多跳问答）的文本上下文来生成一组问题-答案对。问题按两种风格映射答案：开放域问答和多项选择问题。